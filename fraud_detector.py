"""
Credit Card Fraud Detection
===========================

D√©tection de fraude sur cartes de cr√©dit avec approche simple mais efficace.
Dataset: https://www.kaggle.com/mlg-ulb/creditcardfraud

Expertise m√©tier appliqu√©e:
- Gestion du d√©s√©quilibre extr√™me (0.17% de fraude)
- M√©triques orient√©es business (Precision/Recall)
- Analyse des patterns temporels
- Optimisation des seuils pour minimiser les faux positifs

Auteur: Sp√©cialiste en d√©tection de fraude
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, IsolationForest
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (classification_report, confusion_matrix, 
                           roc_auc_score, precision_recall_curve, roc_curve)
from imblearn.over_sampling import SMOTE
from imblearn.under_sampling import RandomUnderSampler
from imblearn.pipeline import Pipeline as ImbPipeline
import joblib
import warnings
warnings.filterwarnings('ignore')

class CreditCardFraudDetector:
    """
    D√©tecteur de fraude optimis√© pour les cartes de cr√©dit.
    
    Fonctionnalit√©s:
    - Preprocessing sp√©cialis√© pour donn√©es tr√®s d√©s√©quilibr√©es
    - Multiple algorithmes (RF, Logistic, Isolation Forest)
    - M√©triques business (co√ªt des faux positifs/n√©gatifs)
    - Optimisation des seuils de d√©cision
    """
    
    def __init__(self, algorithm='random_forest', balance_data=True, verbose=True):
        self.algorithm = algorithm
        self.balance_data = balance_data
        self.verbose = verbose
        self.model = None
        self.scaler = None
        self.feature_names = None
        self.metrics = {}
        
        if verbose:
            print(f"üîß Fraud Detector initialis√©: {algorithm}")
    
    def load_data(self, filepath):
        """
        Charge le dataset Credit Card Fraud depuis Kaggle.
        
        Structure attendue:
        - Time: secondes √©coul√©es depuis premi√®re transaction
        - V1-V28: features anonymis√©es (PCA)
        - Amount: montant de la transaction
        - Class: 0=Normal, 1=Fraude
        """
        try:
            df = pd.read_csv(filepath)
            
            if self.verbose:
                print(f"üìä Dataset charg√©: {len(df):,} transactions")
                print(f"   P√©riode: {df['Time'].max()/3600:.1f} heures")
                print(f"   Fraudes: {df['Class'].sum():,} ({df['Class'].mean():.3%})")
                print(f"   Montant moyen: ${df['Amount'].mean():.2f}")
            
            return df
            
        except FileNotFoundError:
            print("‚ùå Fichier non trouv√©. T√©l√©chargez depuis:")
            print("https://www.kaggle.com/mlg-ulb/creditcardfraud")
            return None
    
    def analyze_fraud_patterns(self, df):
        """
        Analyse les patterns de fraude pour guider le feature engineering.
        
        Points cl√©s m√©tier:
        - Patterns temporels (fraude plus fr√©quente certaines heures?)
        - Patterns de montants (petits vs gros montants)
        - Corr√©lations entre features V1-V28
        """
        print("üîç Analyse des patterns de fraude...")
        
        fraud_data = df[df['Class'] == 1]
        normal_data = df[df['Class'] == 0]
        
        patterns = {}
        
        # 1. Analyse temporelle
        df['Hour'] = (df['Time'] / 3600) % 24
        fraud_by_hour = df.groupby('Hour')['Class'].agg(['count', 'sum', 'mean'])
        patterns['temporal'] = fraud_by_hour
        
        # 2. Analyse des montants
        patterns['amount'] = {
            'fraud_mean': fraud_data['Amount'].mean(),
            'normal_mean': normal_data['Amount'].mean(),
            'fraud_median': fraud_data['Amount'].median(),
            'normal_median': normal_data['Amount'].median()
        }
        
        # 3. Features les plus discriminantes
        feature_importance = {}
        for col in [c for c in df.columns if c.startswith('V')]:
            # Diff√©rence de moyennes entre fraude et normal
            fraud_mean = fraud_data[col].mean()
            normal_mean = normal_data[col].mean()
            feature_importance[col] = abs(fraud_mean - normal_mean)
        
        patterns['top_features'] = sorted(feature_importance.items(), 
                                        key=lambda x: x[1], reverse=True)[:10]
        
        if self.verbose:
            print("‚úÖ Patterns analys√©s:")
            print(f"   Montant moyen fraude: ${patterns['amount']['fraud_mean']:.2f}")
            print(f"   Montant moyen normal: ${patterns['amount']['normal_mean']:.2f}")
            print(f"   Top 3 features: {[f[0] for f in patterns['top_features'][:3]]}")
        
        return patterns
    
    def create_features(self, df):
        """
        Feature engineering sp√©cialis√© pour la fraude carte de cr√©dit.
        
        Features ajout√©es:
        - Heure de la transaction (patterns temporels)
        - Montant normalis√© et cat√©goris√©
        - Interactions entre top features
        - Ratios et transformations m√©tier
        """
        df_enhanced = df.copy()
        
        if self.verbose:
            print("üîß Feature engineering...")
        
        # 1. Features temporelles
        df_enhanced['Hour'] = (df_enhanced['Time'] / 3600) % 24
        df_enhanced['Day'] = (df_enhanced['Time'] / (3600 * 24))
        
        # Patterns temporels business
        df_enhanced['Is_Night'] = ((df_enhanced['Hour'] >= 23) | (df_enhanced['Hour'] <= 6)).astype(int)
        df_enhanced['Is_Weekend'] = (df_enhanced['Day'] % 7 >= 5).astype(int)
        df_enhanced['Is_Business_Hours'] = (
            (df_enhanced['Hour'] >= 9) & (df_enhanced['Hour'] <= 17)
        ).astype(int)
        
        # 2. Features de montant
        df_enhanced['Amount_Log'] = np.log1p(df_enhanced['Amount'])
        df_enhanced['Amount_Sqrt'] = np.sqrt(df_enhanced['Amount'])
        
        # Cat√©gories m√©tier
        df_enhanced['Small_Amount'] = (df_enhanced['Amount'] <= 10).astype(int)
        df_enhanced['Medium_Amount'] = ((df_enhanced['Amount'] > 10) & 
                                      (df_enhanced['Amount'] <= 100)).astype(int)
        df_enhanced['Large_Amount'] = (df_enhanced['Amount'] > 100).astype(int)
        df_enhanced['Round_Amount'] = (df_enhanced['Amount'] % 10 == 0).astype(int)
        
        # 3. Interactions entre top features (V14, V4, V11 souvent importants)
        if all(col in df_enhanced.columns for col in ['V14', 'V4']):
            df_enhanced['V14_V4_Interaction'] = df_enhanced['V14'] * df_enhanced['V4']
        
        if all(col in df_enhanced.columns for col in ['V14', 'V11']):
            df_enhanced['V14_V11_Ratio'] = np.where(
                df_enhanced['V11'] != 0, 
                df_enhanced['V14'] / df_enhanced['V11'], 
                0
            )
        
        # 4. Features d'agr√©gation temporelle
        df_enhanced['Time_Delta'] = df_enhanced['Time'].diff().fillna(0)
        df_enhanced['Frequent_User'] = (df_enhanced['Time_Delta'] < 300).astype(int)  # < 5 min
        
        if self.verbose:
            new_features = len(df_enhanced.columns) - len(df.columns)
            print(f"   ‚úÖ {new_features} nouvelles features cr√©√©es")
        
        return df_enhanced
    
    def prepare_data(self, df, test_size=0.2):
        """
        Pr√©pare les donn√©es pour l'entra√Ænement.
        
        Division temporelle importante en fraude:
        - Train: premi√®res transactions (√©vite data leakage)
        - Test: derni√®res transactions (simule production)
        """
        # Features et target
        feature_cols = [col for col in df.columns if col not in ['Class', 'Time']]
        X = df[feature_cols]
        y = df['Class']
        
        self.feature_names = feature_cols
        
        # Division temporelle plut√¥t qu'al√©atoire
        df_sorted = df.sort_values('Time')
        split_idx = int(len(df_sorted) * (1 - test_size))
        
        X_train = X.iloc[:split_idx]
        X_test = X.iloc[split_idx:]
        y_train = y.iloc[:split_idx]
        y_test = y.iloc[split_idx:]
        
        if self.verbose:
            print(f"üìä Division temporelle des donn√©es:")
            print(f"   Train: {len(X_train):,} ({y_train.mean():.3%} fraude)")
            print(f"   Test:  {len(X_test):,} ({y_test.mean():.3%} fraude)")
        
        return X_train, X_test, y_train, y_test
    
    def train(self, X_train, y_train):
        """
        Entra√Æne le mod√®le avec gestion du d√©s√©quilibre.
        
        Strat√©gies pour donn√©es tr√®s d√©s√©quilibr√©es:
        - SMOTE pour augmenter les fraudes
        - Random undersampling pour r√©duire les normaux
        - Class weights pour p√©naliser les erreurs
        """
        if self.verbose:
            print(f"ü§ñ Entra√Ænement {self.algorithm}...")
        
        # Normalisation des features
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        # Gestion du d√©s√©quilibre
        if self.balance_data and self.algorithm != 'isolation_forest':
            if self.verbose:
                print("   ‚öñÔ∏è R√©√©quilibrage des donn√©es...")
            
            # Pipeline SMOTE + Undersampling
            over = SMOTE(sampling_strategy=0.3, random_state=42)  # 30% de fraudes
            under = RandomUnderSampler(sampling_strategy=0.7, random_state=42)  # 70% normaux
            
            pipeline = ImbPipeline([('over', over), ('under', under)])
            X_resampled, y_resampled = pipeline.fit_resample(X_train_scaled, y_train)
            
            if self.verbose:
                print(f"      Avant: {len(y_train):,} ({y_train.mean():.3%} fraude)")
                print(f"      Apr√®s: {len(y_resampled):,} ({y_resampled.mean():.3%} fraude)")
        else:
            X_resampled, y_resampled = X_train_scaled, y_train
        
        # Initialisation du mod√®le
        if self.algorithm == 'random_forest':
            self.model = RandomForestClassifier(
                n_estimators=100,
                max_depth=10,
                min_samples_split=5,
                class_weight='balanced' if not self.balance_data else None,
                random_state=42,
                n_jobs=-1
            )
        elif self.algorithm == 'logistic':
            self.model = LogisticRegression(
                class_weight='balanced' if not self.balance_data else None,
                random_state=42,
                max_iter=1000
            )
        elif self.algorithm == 'isolation_forest':
            contamination_rate = y_train.mean()  # Pourcentage de fraudes
            self.model = IsolationForest(
                contamination=contamination_rate,
                random_state=42,
                n_jobs=-1
            )
        
        # Entra√Ænement
        if self.algorithm == 'isolation_forest':
            # Isolation Forest: entra√Æner seulement sur donn√©es normales
            normal_data = X_train_scaled[y_train == 0]
            self.model.fit(normal_data)
        else:
            self.model.fit(X_resampled, y_resampled)
        
        if self.verbose:
            print("‚úÖ Mod√®le entra√Æn√©!")
    
    def predict(self, X_test, threshold=0.5):
        """Pr√©dictions avec seuil personnalisable."""
        X_test_scaled = self.scaler.transform(X_test)
        
        if self.algorithm == 'isolation_forest':
            # -1 = anomalie (fraude), 1 = normal
            predictions = self.model.predict(X_test_scaled)
            return (predictions == -1).astype(int)
        else:
            probas = self.model.predict_proba(X_test_scaled)[:, 1]
            return (probas >= threshold).astype(int)
    
    def predict_proba(self, X_test):
        """Probabilit√©s de fraude."""
        X_test_scaled = self.scaler.transform(X_test)
        
        if self.algorithm == 'isolation_forest':
            # Convertir scores d'anomalie en probabilit√©s
            scores = self.model.decision_function(X_test_scaled)
            # Normaliser entre 0 et 1
            probas = (scores.max() - scores) / (scores.max() - scores.min())
            return probas
        else:
            return self.model.predict_proba(X_test_scaled)[:, 1]
    
    def evaluate(self, X_test, y_test, threshold=0.5):
        """
        √âvaluation compl√®te avec m√©triques business.
        
        M√©triques importantes en fraude:
        - AUC-ROC: performance g√©n√©rale
        - Precision: % de vraies fraudes parmi les alertes
        - Recall: % de fraudes d√©tect√©es
        - Co√ªt business: faux positifs vs faux n√©gatifs
        """
        predictions = self.predict(X_test, threshold)
        probabilities = self.predict_proba(X_test)
        
        # M√©triques de base
        auc = roc_auc_score(y_test, probabilities)
        cm = confusion_matrix(y_test, predictions)
        
        tn, fp, fn, tp = cm.ravel()
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0
        
        # M√©triques business
        false_positive_rate = fp / (fp + tn) if (fp + tn) > 0 else 0
        fraud_detection_rate = tp / (tp + fn) if (tp + fn) > 0 else 0
        
        # Co√ªt estim√© (exemple: investigation = 25‚Ç¨, fraude = 500‚Ç¨)
        investigation_cost = 25
        fraud_cost = 500
        
        cost_fp = fp * investigation_cost  # Co√ªt fausses alertes
        cost_fn = fn * fraud_cost         # Co√ªt fraudes rat√©es
        total_cost = cost_fp + cost_fn
        
        savings = tp * fraud_cost - cost_fp  # √âconomies nettes
        roi = (savings / cost_fp * 100) if cost_fp > 0 else 0
        
        self.metrics = {
            'auc': auc,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'specificity': specificity,
            'false_positive_rate': false_positive_rate,
            'fraud_detection_rate': fraud_detection_rate,
            'confusion_matrix': cm,
            'business': {
                'cost_false_positives': cost_fp,
                'cost_false_negatives': cost_fn,
                'total_cost': total_cost,
                'savings': savings,
                'roi': roi
            }
        }
        
        if self.verbose:
            print("üìä R√©sultats d'√©valuation:")
            print(f"   AUC: {auc:.4f}")
            print(f"   Pr√©cision: {precision:.4f}")
            print(f"   Rappel: {recall:.4f}")
            print(f"   F1-Score: {f1:.4f}")
            print(f"   Taux d√©tection fraude: {fraud_detection_rate:.2%}")
            print(f"   Taux faux positifs: {false_positive_rate:.2%}")
            print(f"\nüí∞ Impact business:")
            print(f"   ROI: {roi:.1f}%")
            print(f"   √âconomies: {savings:,.0f}‚Ç¨")
        
        return self.metrics
    
    def find_optimal_threshold(self, X_test, y_test, metric='f1'):
        """
        Trouve le seuil optimal selon la m√©trique business choisie.
        
        Options:
        - 'f1': maximise F1-score
        - 'precision': maximise pr√©cision
        - 'business': minimise co√ªt total
        """
        probabilities = self.predict_proba(X_test)
        
        precision_scores, recall_scores, thresholds = precision_recall_curve(y_test, probabilities)
        
        if metric == 'f1':
            f1_scores = 2 * (precision_scores * recall_scores) / (precision_scores + recall_scores)
            f1_scores = np.nan_to_num(f1_scores)
            optimal_idx = np.argmax(f1_scores)
            optimal_threshold = thresholds[optimal_idx]
            
        elif metric == 'precision':
            # Seuil qui donne pr√©cision >= 95%
            high_precision_idx = np.where(precision_scores >= 0.95)[0]
            if len(high_precision_idx) > 0:
                optimal_idx = high_precision_idx[0]
                optimal_threshold = thresholds[optimal_idx]
            else:
                optimal_threshold = 0.9  # Seuil conservateur
                
        elif metric == 'business':
            # Minimiser co√ªt total
            costs = []
            for threshold in thresholds:
                preds = (probabilities >= threshold).astype(int)
                cm = confusion_matrix(y_test, preds)
                tn, fp, fn, tp = cm.ravel()
                cost = fp * 25 + fn * 500  # Co√ªts exemple
                costs.append(cost)
            
            optimal_idx = np.argmin(costs)
            optimal_threshold = thresholds[optimal_idx]
        
        if self.verbose:
            print(f"üéØ Seuil optimal ({metric}): {optimal_threshold:.4f}")
        
        return optimal_threshold
    
    def get_feature_importance(self, top_n=20):
        """Importance des features pour interpr√©tation."""
        if hasattr(self.model, 'feature_importances_'):
            importance_df = pd.DataFrame({
                'feature': self.feature_names,
                'importance': self.model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            return importance_df.head(top_n)
        else:
            if self.verbose:
                print("‚ö†Ô∏è Feature importance non disponible pour ce mod√®le")
            return None
    
    def save_model(self, filepath):
        """Sauvegarde le mod√®le complet."""
        model_data = {
            'model': self.model,
            'scaler': self.scaler,
            'feature_names': self.feature_names,
            'algorithm': self.algorithm,
            'metrics': self.metrics
        }
        joblib.dump(model_data, filepath)
        
        if self.verbose:
            print(f"üíæ Mod√®le sauvegard√©: {filepath}")
    
    def load_model(self, filepath):
        """Charge un mod√®le pr√©-entra√Æn√©."""
        model_data = joblib.load(filepath)
        
        self.model = model_data['model']
        self.scaler = model_data['scaler']
        self.feature_names = model_data['feature_names']
        self.algorithm = model_data['algorithm']
        self.metrics = model_data.get('metrics', {})
        
        if self.verbose:
            print(f"üìÇ Mod√®le charg√©: {filepath}")


def plot_evaluation_results(detector, X_test, y_test):
    """
    Graphiques d'√©valuation du mod√®le.
    """
    probabilities = detector.predict_proba(X_test)
    
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('√âvaluation du Mod√®le de D√©tection de Fraude', fontsize=16, fontweight='bold')
    
    # 1. Distribution des scores
    axes[0,0].hist(probabilities[y_test == 0], bins=50, alpha=0.7, label='Normal', density=True)
    axes[0,0].hist(probabilities[y_test == 1], bins=50, alpha=0.7, label='Fraude', density=True)
    axes[0,0].set_xlabel('Score de Probabilit√©')
    axes[0,0].set_ylabel('Densit√©')
    axes[0,0].set_title('Distribution des Scores')
    axes[0,0].legend()
    
    # 2. ROC Curve
    fpr, tpr, _ = roc_curve(y_test, probabilities)
    auc = roc_auc_score(y_test, probabilities)
    axes[0,1].plot(fpr, tpr, label=f'AUC = {auc:.4f}')
    axes[0,1].plot([0, 1], [0, 1], 'k--', alpha=0.5)
    axes[0,1].set_xlabel('Taux de Faux Positifs')
    axes[0,1].set_ylabel('Taux de Vrais Positifs')
    axes[0,1].set_title('Courbe ROC')
    axes[0,1].legend()
    
    # 3. Precision-Recall Curve
    precision, recall, thresholds = precision_recall_curve(y_test, probabilities)
    axes[1,0].plot(recall, precision)
    axes[1,0].set_xlabel('Rappel')
    axes[1,0].set_ylabel('Pr√©cision')
    axes[1,0].set_title('Courbe Pr√©cision-Rappel')
    
    # 4. Feature Importance
    importance = detector.get_feature_importance(15)
    if importance is not None:
        axes[1,1].barh(range(len(importance)), importance['importance'])
        axes[1,1].set_yticks(range(len(importance)))
        axes[1,1].set_yticklabels(importance['feature'])
        axes[1,1].set_xlabel('Importance')
        axes[1,1].set_title('Top 15 Features Importantes')
    
    plt.tight_layout()
    plt.show()


def run_complete_analysis(data_path, algorithm='random_forest'):
    """
    Pipeline complet d'analyse de fraude.
    
    √âtapes:
    1. Chargement et analyse des donn√©es
    2. Feature engineering
    3. Entra√Ænement du mod√®le
    4. √âvaluation et optimisation
    5. Visualisations
    """
    print("üéØ Analyse Compl√®te de Fraude - Credit Card")
    print("=" * 50)
    
    # Initialisation
    detector = CreditCardFraudDetector(algorithm=algorithm, verbose=True)
    
    # Chargement
    df = detector.load_data(data_path)
    if df is None:
        return None
    
    # Analyse des patterns
    patterns = detector.analyze_fraud_patterns(df)
    
    # Feature engineering
    df_enhanced = detector.create_features(df)
    
    # Pr√©paration des donn√©es
    X_train, X_test, y_train, y_test = detector.prepare_data(df_enhanced)
    
    # Entra√Ænement
    detector.train(X_train, y_train)
    
    # √âvaluation
    results = detector.evaluate(X_test, y_test)
    
    # Optimisation du seuil
    optimal_threshold = detector.find_optimal_threshold(X_test, y_test, metric='business')
    
    # R√©√©valuation avec seuil optimal
    print(f"\nüéØ √âvaluation avec seuil optimal ({optimal_threshold:.4f}):")
    final_results = detector.evaluate(X_test, y_test, threshold=optimal_threshold)
    
    # Visualisations
    plot_evaluation_results(detector, X_test, y_test)
    
    # Sauvegarder le mod√®le
    detector.save_model(f'credit_card_fraud_{algorithm}.pkl')
    
    return detector, final_results


# Exemple d'utilisation
if __name__ == "__main__":
    print("üß™ Test du Credit Card Fraud Detector")
    print("=" * 40)
    
    # Test avec donn√©es synth√©tiques si pas de fichier
    print("Cr√©er des donn√©es d'exemple pour test...")
    
    # Donn√©es synth√©tiques simplifi√©es
    np.random.seed(42)
    n_samples = 10000
    
    # Simuler le dataset Credit Card
    data = {
        'Time': np.random.uniform(0, 172800, n_samples),  # 48h en secondes
        'Amount': np.random.lognormal(3, 1.5, n_samples)
    }
    
    # Ajouter features V1-V28 (simul√©es)
    for i in range(1, 29):
        data[f'V{i}'] = np.random.normal(0, 1, n_samples)
    
    # Cr√©er fraudes r√©alistes
    fraud_indicators = (
        (data['Amount'] > np.percentile(data['Amount'], 95)) |  # Gros montants
        (np.random.random(n_samples) < 0.001)  # Probabilit√© de base
    )
    data['Class'] = fraud_indicators.astype(int)
    
    df_test = pd.DataFrame(data)
    
    print(f"‚úÖ Dataset test: {len(df_test):,} transactions, {df_test['Class'].mean():.3%} fraude")
    
    # Test rapide des fonctions
    detector = CreditCardFraudDetector(algorithm='random_forest', verbose=True)
    df_enhanced = detector.create_features(df_test)
    
    print(f"üìä Features engineering test√©:")
    print(f"   Avant: {len(df_test.columns)} colonnes")
    print(f"   Apr√®s: {len(df_enhanced.columns)} colonnes")
    
    print("\n‚úÖ Code test√© avec succ√®s!")
    print("üöÄ Pr√™t pour les vraies donn√©es Kaggle!")
